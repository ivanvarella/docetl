# DocETL - AnÃ¡lise de Documentos com IA

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![DocETL](https://img.shields.io/badge/DocETL-0.2.5-green.svg)](https://github.com/docetl/docetl)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4o--mini-orange.svg)](https://openai.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ Sobre o Projeto

Este projeto demonstra o uso da biblioteca **DocETL** para anÃ¡lise inteligente de documentos jurÃ­dicos, especificamente a Lei Geral de ProteÃ§Ã£o de Dados (LGPD) do Brasil. O sistema utiliza **Docling** para extraÃ§Ã£o automÃ¡tica de texto de PDFs e **DocETL** com processamento de linguagem natural com IA para extrair, analisar e resumir informaÃ§Ãµes de documentos complexos de forma estruturada e temÃ¡tica.

### ğŸ¯ Objetivos

- **ExtraÃ§Ã£o AutomÃ¡tica**: Utilizar Docling para converter automaticamente PDFs em texto processÃ¡vel
- **Processamento Inteligente**: Utilizar IA para analisar documentos jurÃ­dicos extensos
- **ExtraÃ§Ã£o Estruturada**: Transformar texto nÃ£o estruturado em dados organizados por tÃ³picos
- **AutomaÃ§Ã£o de AnÃ¡lise**: Criar pipelines automatizados para processamento de documentos
- **OrganizaÃ§Ã£o de Dados**: Manter estrutura organizada de arquivos de entrada e saÃ­da
- **DemonstraÃ§Ã£o PrÃ¡tica**: Mostrar as capacidades da biblioteca DocETL em cenÃ¡rios reais

### ğŸš€ Funcionalidades Principais

- **ExtraÃ§Ã£o Inteligente de Documentos**: ConversÃ£o automÃ¡tica de PDFs para texto usando Docling
- **OrganizaÃ§Ã£o Estruturada**: SeparaÃ§Ã£o automÃ¡tica de arquivos em pastas organizadas (PDFs e textos)
- **Split de Documentos**: DivisÃ£o automÃ¡tica de documentos longos em seÃ§Ãµes gerenciÃ¡veis
- **AnÃ¡lise com IA**: Processamento individual de cada seÃ§Ã£o usando GPT-4o-mini
- **Agrupamento Inteligente**: ConsolidaÃ§Ã£o de informaÃ§Ãµes por tÃ³picos relacionados
- **Resumo AutomÃ¡tico**: GeraÃ§Ã£o de sumÃ¡rios temÃ¡ticos consolidados
- **Cache Inteligente**: Sistema de cache para otimizaÃ§Ã£o de custos e performance

## ğŸ› ï¸ Tecnologias Utilizadas

### Core Technologies

- **Python 3.8+** - Linguagem principal
- **DocETL 0.2.5** - Biblioteca de processamento de documentos
- **Docling 2.29.0** - Biblioteca para extraÃ§Ã£o e conversÃ£o de documentos (PDF, DOCX, HTML)
- **OpenAI GPT-4o-mini** - Modelo de IA para anÃ¡lise de texto
- **PyYAML 6.0.2** - ConfiguraÃ§Ã£o de pipelines

### Dependencies Principais

- **Docling 2.29.0** - ExtraÃ§Ã£o e conversÃ£o de documentos
- **PyMuPDF 1.26.4** - Processamento de PDFs
- **Pandas 2.3.2** - ManipulaÃ§Ã£o de dados
- **NumPy 2.3.2** - ComputaÃ§Ã£o numÃ©rica
- **Scikit-learn 1.7.1** - Machine Learning
- **Rich 14.1.0** - Interface de terminal rica

### APIs e ServiÃ§os

- **OpenAI API** - Processamento de linguagem natural
- **Azure Document Intelligence** - OCR e extraÃ§Ã£o de texto (opcional)
- **Hugging Face Hub** - Modelos de IA alternativos

## ğŸ“¦ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.8 ou superior
- pip (gerenciador de pacotes Python)
- Chave de API da OpenAI

### 1. Clone o RepositÃ³rio

```bash
# Via HTTPS
git clone https://github.com/ivanvarella/docetl.git
cd docetl

# Via SSH
git clone git@github.com:ivanvarella/docetl.git
cd docetl
```

### 2. Crie um Ambiente Virtual

#### **Linux/macOS:**

```bash
# Usando uv (recomendado - mais rÃ¡pido)
uv venv docetl_env
source docetl_env/bin/activate

# Usando venv tradicional
python -m venv docetl_env
source docetl_env/bin/activate
```

#### **Windows:**

```cmd
# Usando uv (recomendado - mais rÃ¡pido)
uv venv docetl_env
docetl_env\Scripts\activate

# Usando venv tradicional
python -m venv docetl_env
docetl_env\Scripts\activate
```

### 3. Instale as DependÃªncias

#### **Linux/macOS:**

```bash
# Com uv (recomendado)
uv pip install -r requirements.txt

# Com pip tradicional
pip install -r requirements.txt
```

#### **Windows:**

```cmd
# Com uv (recomendado)
uv pip install -r requirements.txt

# Com pip tradicional
pip install -r requirements.txt
```

### 4. Configure as VariÃ¡veis de Ambiente

#### **Linux/macOS:**

```bash
# Copie o arquivo de exemplo
cp .env_exemple .env

# Edite o arquivo .env com sua chave da OpenAI
nano .env
# ou
code .env
```

#### **Windows:**

```cmd
# Copie o arquivo de exemplo
copy .env_exemple .env

# Edite o arquivo .env com sua chave da OpenAI
notepad .env
# ou
code .env
```

**ConteÃºdo do arquivo .env:**

```env
OPENAI_API_KEY=sua_chave_api_aqui
```

### 5. Verifique a InstalaÃ§Ã£o

#### **Linux/macOS:**

```bash
docetl version
```

#### **Windows:**

```cmd
docetl version
```

## ğŸš€ Como Usar

### ğŸ”„ **Novo Fluxo de Trabalho com Docling**

O projeto agora utiliza um **fluxo automatizado e organizado** para processamento de documentos:

1. **ğŸ“ OrganizaÃ§Ã£o AutomÃ¡tica de Arquivos:**

   - **Entrada:** PDFs sÃ£o colocados na pasta `dados_fonte/pdfs/`
   - **Processamento:** O script `make_dataset_docling.py` extrai automaticamente o texto usando Docling
   - **SaÃ­da Organizada:** Textos extraÃ­dos sÃ£o salvos em `dados_fonte/textos/`
   - **Dataset Final:** JSON Ã© gerado na raiz do projeto para uso pelo DocETL

2. **ğŸ”„ Processamento Automatizado:**
   - **ExtraÃ§Ã£o:** Docling converte PDFs para texto com alta precisÃ£o
   - **EstruturaÃ§Ã£o:** Script organiza automaticamente os arquivos
   - **IntegraÃ§Ã£o:** Dataset JSON Ã© gerado no formato correto para o DocETL

### ExecuÃ§Ã£o BÃ¡sica

#### **Linux/macOS:**

1. **Prepare o Dataset (o arquivo jÃ¡ estÃ¡ preparado, testar com outro arquivo, neste caso faÃ§a as alteraÃ§Ãµes necessÃ¡rias no script antes de executa-lo)**

```bash
# O script agora automatiza todo o processo:
# - Extrai texto do PDF usando Docling
# - Organiza arquivos em pastas estruturadas
# - Gera o dataset JSON automaticamente
python make_dataset_docling.py
```

2. **Execute o Pipeline**

```bash
docetl run pipeline.yaml
```

3. **Verifique os Resultados (os arquivos tambÃ©m jÃ¡ estÃ£o gerados)**

```bash
# Resultados finais
cat lgpd_summary_by_topic.json

# Resultados intermediÃ¡rios
ls intermediate_results/
```

#### **Windows:**

1. **Prepare o Dataset (o arquivo jÃ¡ estÃ¡ preparado, testar com outro arquivo, neste caso faÃ§a as alteraÃ§Ãµes necessÃ¡rias no script antes de executa-lo)**

```cmd
# O script agora automatiza todo o processo:
# - Extrai texto do PDF usando Docling
# - Organiza arquivos em pastas estruturadas
# - Gera o dataset JSON automaticamente
python make_dataset_docling.py
```

2. **Execute o Pipeline**

```cmd
docetl run pipeline.yaml
```

3. **Verifique os Resultados (os arquivos tambÃ©m jÃ¡ estÃ£o gerados)**

```cmd
# Resultados finais
type lgpd_summary_by_topic.json

# Resultados intermediÃ¡rios
dir intermediate_results\
```

### Estrutura do Pipeline

O projeto utiliza um pipeline de 3 etapas principais:

1. **Split** (`split_law_by_article`): Divide o documento em artigos individuais
2. **Map** (`analyze_article`): Analisa cada artigo com IA
3. **Reduce** (`summarize_by_topic`): Agrupa e consolida por tÃ³picos

### ConfiguraÃ§Ã£o Personalizada

Edite o arquivo `pipeline.yaml` para:

- Alterar o modelo de IA
- Modificar prompts de anÃ¡lise
- Ajustar parÃ¢metros de processamento
- Adicionar novas operaÃ§Ãµes

### ğŸ†• **Vantagens do Novo Sistema com Docling**

- **ğŸ”„ AutomaÃ§Ã£o Completa:** NÃ£o Ã© mais necessÃ¡rio extrair manualmente texto de PDFs
- **ğŸ“ OrganizaÃ§Ã£o Estruturada:** Arquivos sÃ£o automaticamente organizados em pastas lÃ³gicas
- **ğŸ”§ Flexibilidade:** FÃ¡cil troca de PDFs de entrada - apenas coloque na pasta `dados_fonte/pdfs/`
- **ğŸ“Š IntegraÃ§Ã£o Perfeita:** Dataset JSON Ã© gerado automaticamente no formato correto para o DocETL
- **âš¡ EficiÃªncia:** Processo de extraÃ§Ã£o e preparaÃ§Ã£o Ã© feito em uma Ãºnica execuÃ§Ã£o
- **ğŸ›¡ï¸ Robustez:** Tratamento de erros e validaÃ§Ãµes automÃ¡ticas em cada etapa

## ğŸ“Š Estrutura do Projeto

```
.
â”œâ”€â”€ ğŸ“„ converter_utf8.py         # UtilitÃ¡rio para correÃ§Ã£o de codificaÃ§Ã£o UTF-8
â”œâ”€â”€ ğŸ“ dados_fonte/              # Pasta organizada com PDFs e textos extraÃ­dos
â”‚   â”œâ”€â”€ ğŸ“ pdfs/                 # PDFs de entrada para processamento
â”‚   â”‚   â””â”€â”€ L13709compilado.pdf  # PDF da Lei Geral de ProteÃ§Ã£o de Dados (LGPD)
â”‚   â””â”€â”€ ğŸ“ textos/               # Arquivos de texto extraÃ­dos automaticamente
â”‚       â””â”€â”€ L13709compilado.pdf_TEXT.txt # Texto extraÃ­do do PDF pela biblioteca Docling
â”œâ”€â”€ ğŸ“ intermediate_results/      # Resultados intermediÃ¡rios do pipeline DocETL
â”‚   â”œâ”€â”€ ğŸ“ law_analysis_step/    # Resultados da etapa de anÃ¡lise dos artigos
â”‚   â”‚   â”œâ”€â”€ analyze_article.json # AnÃ¡lises individuais de cada artigo da LGPD
â”‚   â”‚   â””â”€â”€ split_law_by_article.json # DivisÃ£o do documento em artigos individuais
â”‚   â””â”€â”€ ğŸ“ topic_summary_step/   # Resultados da etapa de resumo por tÃ³picos
â”‚       â””â”€â”€ summarize_by_topic.json # Resumos consolidados por grupos temÃ¡ticos
â”œâ”€â”€ ğŸ“„ lgpd_dataset.json         # Dataset de entrada processado para o pipeline DocETL
â”œâ”€â”€ ğŸ“„ lgpd_summary_by_topic.json # Resultado final do pipeline com resumos temÃ¡ticos
â”œâ”€â”€ ğŸ“„ make_dataset_docling.py   # Script principal que automatiza extraÃ§Ã£o com Docling e geraÃ§Ã£o do dataset
â”œâ”€â”€ ğŸ“„ pipeline.yaml             # ConfiguraÃ§Ã£o do pipeline DocETL em formato YAML
â”œâ”€â”€ ğŸ“„ .env_exemple              # Arquivo de exemplo para configuraÃ§Ã£o de variÃ¡veis de ambiente
â”œâ”€â”€ ğŸ“„ README.md                 # DocumentaÃ§Ã£o completa do projeto
â”œâ”€â”€ ğŸ“„ RelatÃ³rio_DocETL.pdf      # RelatÃ³rio tÃ©cnico detalhado da anÃ¡lise da LGPD
â”œâ”€â”€ ğŸ“„ requirements.txt          # Lista de dependÃªncias Python do projeto
â””â”€â”€ ğŸ“„ tree.txt                  # Estrutura de diretÃ³rios do projeto
```

### ğŸ“‹ **DescriÃ§Ã£o Detalhada dos Arquivos**

#### ğŸ”§ **Scripts e UtilitÃ¡rios**

- **`make_dataset_docling.py`**: Script principal que automatiza todo o processo de extraÃ§Ã£o de dados. Utiliza a biblioteca Docling para converter PDFs em texto, organiza arquivos em pastas estruturadas, e gera automaticamente o dataset JSON no formato correto para o DocETL.

- **`converter_utf8.py`**: UtilitÃ¡rio para correÃ§Ã£o de codificaÃ§Ã£o de caracteres. Resolve problemas de visualizaÃ§Ã£o de acentos e caracteres especiais em arquivos JSON, convertendo-os para UTF-8 adequado.

#### ğŸ“ **Pasta dados_fonte/**

- **`dados_fonte/pdfs/`**: ContÃ©m os PDFs de entrada para processamento. Atualmente contÃ©m o arquivo da LGPD, mas pode ser facilmente expandido para outros documentos.

- **`dados_fonte/textos/`**: Armazena automaticamente os textos extraÃ­dos dos PDFs. Os arquivos sÃ£o nomeados seguindo o padrÃ£o `[nome_original]_TEXT.txt`.

#### ğŸ“Š **Resultados do Pipeline**

- **`lgpd_dataset.json`**: Dataset de entrada para o pipeline DocETL. ContÃ©m o texto extraÃ­do da LGPD no formato estruturado esperado pela biblioteca.

- **`lgpd_summary_by_topic.json`**: Resultado final do pipeline, contendo resumos temÃ¡ticos consolidados da LGPD. Organiza as informaÃ§Ãµes por 36 tÃ³picos principais identificados automaticamente.

- **`intermediate_results/`**: DiretÃ³rio criado automaticamente pelo DocETL que armazena resultados intermediÃ¡rios de cada operaÃ§Ã£o, facilitando depuraÃ§Ã£o e anÃ¡lise do processo.

#### âš™ï¸ **ConfiguraÃ§Ã£o e DocumentaÃ§Ã£o**

- **`pipeline.yaml`**: Arquivo de configuraÃ§Ã£o declarativa que define todo o fluxo de trabalho do DocETL. Especifica operaÃ§Ãµes de split, map e reduce para processamento da LGPD.

- **`.env_exemple`**: Template para configuraÃ§Ã£o de variÃ¡veis de ambiente, incluindo chaves de API necessÃ¡rias para o funcionamento do sistema.

- **`requirements.txt`**: Lista completa de dependÃªncias Python com versÃµes especÃ­ficas, incluindo Docling 2.29.0, DocETL e outras bibliotecas necessÃ¡rias.

- **`README.md`**: DocumentaÃ§Ã£o completa do projeto, incluindo instruÃ§Ãµes de instalaÃ§Ã£o, uso e explicaÃ§Ã£o detalhada do fluxo de trabalho.

- **`RelatÃ³rio_DocETL.pdf`**: RelatÃ³rio tÃ©cnico detalhado documentando todo o processo de anÃ¡lise da LGPD, incluindo metodologia, resultados e reflexÃµes sobre o uso da biblioteca DocETL.

- **`tree.txt`**: RepresentaÃ§Ã£o textual da estrutura de diretÃ³rios do projeto para referÃªncia rÃ¡pida.

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### OtimizaÃ§Ã£o de Performance

#### **Linux/macOS:**

```bash
# Usar otimizador automÃ¡tico
docetl build pipeline.yaml

# Executar com cache limpo
docetl run pipeline.yaml --clear-cache
```

#### **Windows:**

```cmd
# Usar otimizador automÃ¡tico
docetl build pipeline.yaml

# Executar com cache limpo
docetl run pipeline.yaml --clear-cache
```

### Logs e Debugging

#### **Linux/macOS:**

```bash
# ExecuÃ§Ã£o com logs detalhados
docetl run pipeline.yaml --verbose

# Verificar status do cache
docetl cache status
```

#### **Windows:**

```cmd
# ExecuÃ§Ã£o com logs detalhados
docetl run pipeline.yaml --verbose

# Verificar status do cache
docetl cache status
```

### PersonalizaÃ§Ã£o de Modelos

Edite o `pipeline.yaml` para usar diferentes modelos:

```yaml
default_model: gpt-4o-mini # ou gpt-4, gpt-3.5-turbo, etc.
```

## ğŸ“ˆ MÃ©tricas e Performance

### EstatÃ­sticas de ExecuÃ§Ã£o

- **Documentos Processados**: 57 artigos da LGPD
- **TÃ³picos Identificados**: 36 grupos temÃ¡ticos
- **Chamadas Ã  API**: 93 total (57 map + 36 reduce)
- **Tokens Consumidos**: 62.732 (54.337 input + 8.395 output)
- **Custo Estimado**: ~$0.01 por execuÃ§Ã£o completa

### OtimizaÃ§Ãµes Implementadas

- **Sistema de Cache**: ReutilizaÃ§Ã£o de resultados processados
- **Processamento em Lotes**: Agrupamento eficiente de operaÃ§Ãµes
- **ValidaÃ§Ã£o de Schema**: Estrutura de dados consistente

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

</br></br></br></br>

---

# RelatÃ³rio de AnÃ¡lise da LGPD com a Biblioteca DocETL

**Data:** 29 de agosto de 2025
**ResponsÃ¡vel:** Ivan Varella
**Projeto:** Teste da biblioteca DocETL para anÃ¡lise de texto jurÃ­dico (Lei Geral de ProteÃ§Ã£o de Dados - LGPD).

## 1. IntroduÃ§Ã£o

Este relatÃ³rio documenta o processo de utilizaÃ§Ã£o da biblioteca de processamento de documentos `DocETL` para realizar uma anÃ¡lise estruturada e temÃ¡tica da Lei Geral de ProteÃ§Ã£o de Dados do Brasil (LGPD, LEI NÂº 13.709). O objetivo foi testar as capacidades da biblioteca em um cenÃ¡rio real e complexo, transformando um longo e denso texto jurÃ­dico em um sumÃ¡rio organizado por temas.

O projeto partiu de uma configuraÃ§Ã£o inicial robusta. Primeiramente, o texto da lei foi extraÃ­do de seu arquivo PDF original utilizando a biblioteca `Docling` atravÃ©s do script `make_dataset_docling.py`. Este script automatiza todo o processo: extrai o texto do PDF, salva na pasta organizada `dados_fonte/textos/`, e gera automaticamente o arquivo de dataset `lgpd_dataset.json` no formato esperado pela biblioteca `DocETL`. A partir deste ponto, com o ambiente Python devidamente configurado com as chaves de API da OpenAI, a metodologia adotada envolveu a criaÃ§Ã£o iterativa de um pipeline em YAML, com depuraÃ§Ã£o e anÃ¡lise dos resultados a cada passo.

## 2. Metodologia e ExecuÃ§Ã£o do Pipeline

Para atingir o objetivo, foi desenhado um pipeline de mÃºltiplas etapas, aproveitando os principais operadores da DocETL para decompor a tarefa complexa.

### Arquitetura do Pipeline: `Split -> Map -> Reduce`

O pipeline foi estruturado em trÃªs operaÃ§Ãµes lÃ³gicas principais, orquestradas em dois passos sequenciais:

1. **OperaÃ§Ã£o `split_law_by_article` (Split):**
   - **Objetivo:** Dividir o texto monolÃ­tico da LGPD em unidades de anÃ¡lise menores e mais gerenciÃ¡veis.
   - **ExecuÃ§Ã£o:** Utilizou-se o delimitador `"- Art."` para quebrar o documento original. Esta aÃ§Ã£o resultou em **57 chunks**, cada um correspondendo a um artigo ou parÃ¡grafo da lei.
2. **OperaÃ§Ã£o `analyze_article` (Map):**
   - **Objetivo:** Analisar cada um dos 57 chunks individualmente para extrair informaÃ§Ãµes estruturadas.
   - **ExecuÃ§Ã£o:** Para cada chunk, foi feita uma chamada Ã  API da OpenAI (modelo `gpt-4o-mini`). O prompt instruiu o modelo a identificar o **tÃ³pico principal** do artigo e a gerar um **resumo conciso** de seu conteÃºdo.
3. **OperaÃ§Ã£o `summarize_by_topic` (Reduce):**
   - **Objetivo:** Agrupar os artigos analisados por tema e criar um resumo consolidado para cada um.
   - **ExecuÃ§Ã£o:** A operaÃ§Ã£o utilizou a chave `topic` (gerada na etapa anterior) para agrupar os 57 itens. Isso resultou em **36 grupos de tÃ³picos Ãºnicos**. Para cada grupo, uma nova chamada Ã  API foi realizada, solicitando a sÃ­ntese de todos os resumos individuais em um parÃ¡grafo final coeso.

### Dataset de Entrada (`lgpd_dataset.json`)

O ponto de partida para o pipeline foi um arquivo JSON simples, contendo um Ãºnico objeto com uma chave `"src"`. O valor dessa chave era o texto completo da LGPD, previamente extraÃ­do e limpo. A estrutura Ã© exemplificada abaixo:

```
[
  {
    "src": "## Mensagem de veto\n\n## VigÃªncia\n\n## PresidÃªncia da RepÃºblica\n\n## Secretaria-Geral Subchefia para Assuntos JurÃ­dicos\n\n## LEI NÂº 13.709, DE 14 DE AGOSTO DE 2018\n\nLei Geral de ProteÃ§Ã£o de Dados Pessoais (LGPD).    (RedaÃ§Ã£o dada pela Lei nÂº 13.853, de 2019) VigÃªncia\n\nO PRESIDENTE DA REPÃšBLICA FaÃ§o saber que o Congresso Nacional decreta e eu sanciono a seguinte Lei:\n\n## CAPÃTULO I DISPOSIÃ‡Ã•ES PRELIMINARES\n\n- Art. 1Âº Esta Lei dispÃµe sobre o tratamento de dados pessoais, inclusive nos meios digitais, por pessoa natural ou por pessoa jurÃ­dica de direito pÃºblico ou privado, com o objetivo de proteger os direitos fundamentais de liberdade e de privacidade e o livre desenvolvimento da personalidade da pessoa natural.\n\n[... restante do texto da lei ...]"
  }
]

```

### ConfiguraÃ§Ã£o do Pipeline (`pipeline.yaml`)

Todo o fluxo de trabalho foi definido no arquivo `pipeline.yaml`. Este arquivo declarativo instrui o DocETL sobre quais dados usar, quais operaÃ§Ãµes executar e em que ordem. O conteÃºdo completo utilizado foi:

```
datasets:
  lgpd_dataset:
    type: file
    path: "lgpd_dataset.json"

default_model: gpt-4o-mini
system_prompt:
  dataset_description: "O conteÃºdo da Lei Geral de ProteÃ§Ã£o de Dados do Brasil (LGPD - LEI NÂº 13.709)."
  persona: "Um assistente jurÃ­dico especialista em legislaÃ§Ã£o brasileira, focado em anÃ¡lise e sÃ­ntese de textos legais."

operations:
  - name: split_law_by_article
    type: split
    split_key: src
    method: delimiter
    method_kwargs:
      delimiter: "- Art."
      num_splits_to_group: 1

  - name: analyze_article
    type: map
    prompt: |
      Analise o seguinte trecho da Lei Geral de ProteÃ§Ã£o de Dados (LGPD) do Brasil:
      "{{ input.src_chunk }}"

      Com base no texto, forneÃ§a as seguintes informaÃ§Ãµes:
      1.  **TÃ³pico Principal**: Identifique e retorne o tema central deste artigo (ex: "PrincÃ­pios", "Direitos do Titular", "Bases Legais", "Tratamento de Dados SensÃ­veis", "SanÃ§Ãµes Administrativas", "TransferÃªncia Internacional de Dados", "Agentes de Tratamento").
      2.  **Resumo**: Crie um resumo conciso (1-2 sentenÃ§as) explicando o propÃ³sito principal deste artigo.
    output:
      schema:
        topic: "string"
        summary: "string"

  - name: summarize_by_topic
    type: reduce
    reduce_key: topic
    prompt: |
      VocÃª recebeu vÃ¡rios artigos da LGPD que tratam sobre o mesmo tÃ³pico: "{{ reduce_key }}".

      Abaixo estÃ£o os resumos de cada artigo relacionado a este tÃ³pico:
      {% for item in inputs %}
      - Artigo: {{ item.summary }}
      {% endfor %}

      Sintetize todas essas informaÃ§Ãµes em um parÃ¡grafo coeso e abrangente que explique como a LGPD aborda o tÃ³pico de "{{ reduce_key }}".
    output:
      schema:
        topic_summary: "string"

pipeline:
  steps:
    - name: law_analysis_step
      input: lgpd_dataset
      operations:
        - split_law_by_article
        - analyze_article

    - name: topic_summary_step
      input: law_analysis_step
      operations:
        - summarize_by_topic

  output:
    type: file
    path: "lgpd_summary_by_topic.json"
    intermediate_dir: "intermediate_results"

```

### Estrutura de Arquivos do Projeto

A execuÃ§Ã£o do pipeline e os scripts de preparaÃ§Ã£o resultaram na seguinte estrutura de arquivos e diretÃ³rios na raiz do projeto:

```
.
â”œâ”€â”€ converter_utf8.py
â”œâ”€â”€ intermediate_results/
â”‚   â”œâ”€â”€ law_analysis_step/
â”‚   â”‚   â”œâ”€â”€ analyze_article.json
â”‚   â”‚   â””â”€â”€ split_law_by_article.json
â”‚   â””â”€â”€ topic_summary_step/
â”‚       â””â”€â”€ summarize_by_topic.json
â”œâ”€â”€ dados_fonte/
â”‚   â”œâ”€â”€ pdfs/
â”‚   â”‚   â””â”€â”€ L13709compilado.pdf
â”‚   â””â”€â”€ textos/
â”‚       â””â”€â”€ L13709compilado_TEXT.txt
â”œâ”€â”€ lgpd_dataset.json
â”œâ”€â”€ lgpd_summary_by_topic.json
â”œâ”€â”€ make_dataset_docling.py
â””â”€â”€ pipeline.yaml

```

- **`pipeline.yaml`**: Arquivo de configuraÃ§Ã£o que define todas as etapas e operaÃ§Ãµes do pipeline do DocETL.
- **`make_dataset_docling.py`**: Script principal que utiliza Docling para extrair texto de PDFs e gerar o dataset JSON.
- **`converter_utf8.py`**: Script utilitÃ¡rio para corrigir a codificaÃ§Ã£o de caracteres do arquivo de saÃ­da.
- **`dados_fonte/`**: Pasta organizada contendo PDFs de entrada e textos extraÃ­dos automaticamente.
- **`lgpd_dataset.json`**: Dataset de entrada para o pipeline, formatado na estrutura correta.
- **`intermediate_results/`**: DiretÃ³rio criado automaticamente pelo DocETL para armazenar os resultados de cada operaÃ§Ã£o, facilitando a depuraÃ§Ã£o.
- **`lgpd_summary_by_topic.json`**: O arquivo final, contendo o resumo temÃ¡tico da LGPD.

## 3. AnÃ¡lise dos Resultados

O pipeline foi executado com sucesso, gerando o arquivo `lgpd_summary_by_topic.json`. A anÃ¡lise do conteÃºdo revelou os seguintes pontos:

### Pontos Positivos

- **Qualidade da SumarizaÃ§Ã£o:** O resultado final Ã© um sumÃ¡rio temÃ¡tico de alta qualidade da LGPD. Os tÃ³picos extraÃ­dos sÃ£o pertinentes e os resumos sÃ£o precisos e bem escritos, demonstrando a eficÃ¡cia do pipeline.
- **EficÃ¡cia do Agrupamento:** O campo `_counts_prereduce_summarize_by_topic`, adicionado pelo DocETL, confirmou o sucesso do agrupamento. Por exemplo:
  - O tÃ³pico **"Agentes de Tratamento"** consolidou informaÃ§Ãµes de **12 artigos** diferentes.
  - O tÃ³pico **"Direitos do Titular"** agrupou **5 artigos**.
  - Isso mostra que o pipeline conseguiu identificar e agrupar com sucesso conteÃºdo relacionado disperso ao longo da lei.

### Pontos de Melhoria Identificados

A anÃ¡lise tambÃ©m revelou uma oportunidade clara de refinamento, causada pela variabilidade natural da geraÃ§Ã£o de texto por LLMs:

- **RedundÃ¢ncia de TÃ³picos:** Foram identificados tÃ³picos semanticamente idÃªnticos, mas textualmente diferentes. Por exemplo:
  - `"Regras de Boas PrÃ¡ticas e GovernanÃ§a em ProteÃ§Ã£o de Dados"` vs. `"Boas PrÃ¡ticas e GovernanÃ§a"`
  - `"Tratamento de Dados Pessoais pelo Poder PÃºblico"` vs. `"Tratamento de Dados pelo Poder PÃºblico"`

Isso fez com que o operador `reduce` os tratasse como grupos distintos, fragmentando o que deveria ser um Ãºnico resumo consolidado.

## 4. AnÃ¡lise da ExecuÃ§Ã£o e Logs

A anÃ¡lise dos logs do terminal foi crucial para depurar e entender o comportamento do DocETL.

### Custo de ExecuÃ§Ã£o

- **ObservaÃ§Ã£o:** O log da execuÃ§Ã£o final bem-sucedida reportou um custo de `$0.00`.
- **AnÃ¡lise:** Isso nÃ£o significa que a API nÃ£o foi usada. O custo de **$0.01** foi, na verdade, incorrido na primeira tentativa de execuÃ§Ã£o para a operaÃ§Ã£o de `map` (processando os 57 chunks). Como o **mecanismo de cache do DocETL** funcionou perfeitamente, na segunda tentativa ele reutilizou os resultados jÃ¡ processados, e executou apenas a etapa de `reduce`, cujo custo foi baixo o suficiente para ser arredondado para zero. Isso confirma que o pipeline estÃ¡ corretamente configurado para usar a API, mas de forma eficiente.

### Detalhes das Chamadas Ã  API e Consumo de Tokens

A execuÃ§Ã£o completa do pipeline (considerando um cache vazio) foi confirmada como tendo realizado um total de **93 chamadas Ã  API** da OpenAI. Essa atividade resultou em um consumo total de **62.732 tokens**, divididos em **54.337 tokens de entrada (input)** e **8.395 tokens de saÃ­da (output)**.

Essa distribuiÃ§Ã£o de tokens e chamadas pode ser explicada da seguinte forma:

- **57 Chamadas na Etapa `Map`:** A operaÃ§Ã£o `analyze_article` processou cada um dos **57 chunks** de texto. Para cada chamada, os tokens de entrada consistiram no texto do artigo da lei mais as instruÃ§Ãµes do prompt. Os tokens de saÃ­da foram o JSON estruturado com o tÃ³pico e o resumo. Esta etapa foi responsÃ¡vel pela maior parte dos tokens de entrada.
- **36 Chamadas na Etapa `Reduce`:** A operaÃ§Ã£o `summarize_by_topic` foi executada para cada um dos **36 tÃ³picos Ãºnicos** identificados. Para cada chamada, os tokens de entrada foram os resumos dos artigos daquele tÃ³pico mais as instruÃ§Ãµes do prompt de sÃ­ntese. Os tokens de saÃ­da foram o parÃ¡grafo final consolidado.

![Imagem do Dashboard de gerenciamento da API OpenAI](Documentation/OpenAI.png)

Imagem do Dashboard de gerenciamento da API OpenAI

### CodificaÃ§Ã£o de Caracteres

- **ObservaÃ§Ã£o:** O arquivo JSON inicial apresentava caracteres acentuados no formato de escape Unicode (ex: `Prote\u00e7\u00e3o`).
- **AnÃ¡lise:** O problema de visualizaÃ§Ã£o dos caracteres foi solucionado com a utilizaÃ§Ã£o de um simples script para gravar novamente o arquivo de resultado, especificando a codificaÃ§Ã£o correta (`UTF-8`). Isso corrigiu a representaÃ§Ã£o dos caracteres especiais, resolvendo os problemas de codificaÃ§Ã£o.

### MÃ©tricas de ExecuÃ§Ã£o (57 vs. 36)

- **`57/57` (Map):** Corresponde ao nÃºmero total de **chunks (artigos)** gerados pela operaÃ§Ã£o `split` e processados individualmente pela operaÃ§Ã£o `map`.
- **`36/36` (Reduce):** Corresponde ao nÃºmero de **tÃ³picos Ãºnicos** que foram identificados na etapa de `map`. A operaÃ§Ã£o `reduce` entÃ£o processou cada um desses 36 grupos.

## 5. ConclusÃ£o

O experimento foi um **sucesso**. Demonstrou-se que a biblioteca DocETL Ã© uma ferramenta poderosa e adequada para tarefas complexas de processamento de documentos. O pipeline `Split -> Map -> Reduce` foi capaz de transformar com sucesso um documento nÃ£o estruturado em um sumÃ¡rio temÃ¡tico e organizado. A anÃ¡lise detalhada dos logs e dos resultados nÃ£o sÃ³ validou a eficÃ¡cia do processo, mas tambÃ©m revelou oportunidades claras para otimizaÃ§Ã£o e aprofundamento da anÃ¡lise.

## 6. ReflexÃµes sobre a Ferramenta DocETL

Este teste prÃ¡tico permitiu uma avaliaÃ§Ã£o aprofundada da biblioteca DocETL, revelando seus pontos fortes e desafios.

- **Pontos Fortes:**
  - **Modularidade:** A abordagem baseada em operadores (`split`, `map`, `reduce`) Ã© intuitiva e poderosa, permitindo quebrar problemas complexos em etapas lÃ³gicas e gerenciÃ¡veis.
  - **Cache Inteligente:** O sistema de cache Ã© um dos recursos mais impactantes, economizando significativamente tempo e custos de API durante o desenvolvimento e a depuraÃ§Ã£o iterativa do pipeline.
  - **Interface Declarativa:** O uso de YAML para definir o pipeline torna a configuraÃ§Ã£o clara, legÃ­vel e fÃ¡cil de versionar, sem a necessidade de escrever cÃ³digo complexo para orquestraÃ§Ã£o.
- **Desafios e Aprendizados:**
  - **Curva de Aprendizagem da Sintaxe:** A sintaxe do YAML, especialmente na estruturaÃ§Ã£o de passos (`steps`) e saÃ­das (`output`), requer atenÃ§Ã£o aos detalhes, como visto nos erros iniciais de parsing.
  - **Gerenciamento de SaÃ­da:** A codificaÃ§Ã£o de caracteres do arquivo de saÃ­da JSON precisou de uma intervenÃ§Ã£o externa, indicando uma Ã¡rea onde a ferramenta poderia oferecer mais controle direto.

## 7. PrÃ³ximos Passos e Futuras OtimizaÃ§Ãµes

Para refinar ainda mais os resultados e explorar todo o potencial da ferramenta, os seguintes passos sÃ£o recomendados:

### 7.1. ImplementaÃ§Ã£o do Operador `resolve`

Para solucionar a redundÃ¢ncia de tÃ³picos, a prÃ³xima iteraÃ§Ã£o do pipeline deve incluir o operador `resolve` para unificar tÃ³picos semanticamente equivalentes.

- **Nova Arquitetura:** `Split -> Map -> **Resolve** -> Reduce`
- **Exemplo de OperaÃ§Ã£o `resolve`:**

```
# A ser inserido na seÃ§Ã£o 'operations' do pipeline.yaml

- name: resolve_topics
  type: resolve
  blocking_keys: # Ajuda a comparar apenas tÃ³picos que jÃ¡ sÃ£o textualmente parecidos
    - topic
  comparison_prompt: |
    Compare os dois tÃ³picos da LGPD abaixo:
    TÃ³pico 1: "{{ input1.topic }}"
    TÃ³pico 2: "{{ input2.topic }}"
    Eles representam o mesmo conceito fundamental? Responda apenas com "True" ou "False".
  resolution_prompt: |
    A partir da lista de tÃ³picos duplicados abaixo, escolha o nome mais claro e conciso para representar o grupo:
    {% for entry in inputs %}
    - {{ entry.topic }}
    {% endfor %}
  output:
    schema:
      topic: "string" # O novo nome canÃ´nico do tÃ³pico

```

### 7.2. ExploraÃ§Ã£o de AnÃ¡lises Mais Profundas

Com a base temÃ¡tica estabelecida, o pipeline pode ser estendido para extrair informaÃ§Ãµes mais granulares, como:

- **ExtraÃ§Ã£o de Entidades:** Modificar a operaÃ§Ã£o `map` para extrair entidades especÃ­ficas como prazos (ex: "72 horas"), valores de multas, ou as responsabilidades exatas do Encarregado (DPO).
- **AnÃ¡lise de Sentimento:** Embora menos aplicÃ¡vel a um texto de lei, em outros documentos jurÃ­dicos (como petiÃ§Ãµes ou sentenÃ§as), uma anÃ¡lise de sentimento por clÃ¡usula poderia ser Ãºtil.

### 7.3. Uso do Otimizador (`docetl build`)

Uma abordagem alternativa Ã  criaÃ§Ã£o manual do operador `resolve` seria utilizar o otimizador do DocETL. Executando `docetl build pipeline.yaml`, a ferramenta poderia analisar a relaÃ§Ã£o entre o `map` e o `reduce` e **sintetizar automaticamente** uma operaÃ§Ã£o `resolve` otimizada, incluindo a configuraÃ§Ã£o de `blocking_thresholds` para maior eficiÃªncia.
